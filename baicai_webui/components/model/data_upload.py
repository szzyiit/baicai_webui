import os
from pathlib import Path
from typing import Any, Dict

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import streamlit as st
import torch
from baicai_base.utils.data import get_tmp_folder, load_data
from baicai_dev.utils.data import TaskType, load_example_data
from baicai_dev.utils.setups import (
    bears_func_config,
    bears_re_config,
    bears_single_label_config,
    collab_config,
    create_dl_config,
    create_ml_config,
    garment_config_data,
    house_config_data,
    iris_config_data,
    mnist_csv_config,
    multi_label_config,
    ner_inference_config,
    semantic_match_inference_config,
    sentiment_classifier_trainer_config,
    sentiment_inference_config,
    titanic_config_data,
)

# è®¾ç½®matplotlibä¸­æ–‡æ˜¾ç¤ºï¼Œæ”¯æŒå¤šå¹³å°
plt.rcParams["font.sans-serif"] = [
    "SimHei", "Microsoft YaHei", "Arial Unicode MS", "STHeiti", "PingFang SC", "Heiti TC", "WenQuanYi Micro Hei", "sans-serif"
]
plt.rcParams["axes.unicode_minus"] = False  # æ­£å¸¸æ˜¾ç¤ºè´Ÿå·


@st.cache_data
def create_histogram(df_clean, col_name):
    """åˆ›å»ºç›´æ–¹å›¾çš„ç¼“å­˜å‡½æ•°"""
    try:
        # æ¸…ç†ä¹‹å‰çš„å›¾å½¢
        plt.close("all")

        fig, ax = plt.subplots(figsize=(4, 3))
        df_clean[col_name].hist(ax=ax, bins=20, alpha=0.7, edgecolor="black")
        ax.set_title(f"{col_name}")
        ax.set_xlabel(col_name)
        ax.set_ylabel("é¢‘æ¬¡")
        return fig
    except Exception:
        plt.close("all")  # ç¡®ä¿å‡ºé”™æ—¶ä¹Ÿæ¸…ç†å›¾å½¢
        return None


@st.cache_data
def create_bar_chart(df_clean, col_name):
    """åˆ›å»ºæŸ±çŠ¶å›¾çš„ç¼“å­˜å‡½æ•°"""
    try:
        # æ¸…ç†ä¹‹å‰çš„å›¾å½¢
        plt.close("all")

        value_counts = df_clean[col_name].value_counts().head(10)
        fig, ax = plt.subplots(figsize=(4, 3))
        value_counts.plot(kind="bar", ax=ax, alpha=0.7)
        ax.set_title(f"{col_name}")
        ax.set_xlabel(col_name)
        ax.set_ylabel("é¢‘æ¬¡")
        ax.tick_params(axis="x", rotation=45)
        plt.tight_layout()
        return fig
    except Exception:
        plt.close("all")  # ç¡®ä¿å‡ºé”™æ—¶ä¹Ÿæ¸…ç†å›¾å½¢
        return None


@st.cache_data
def create_scatter_plot_simple(df_clean, x_col, y_col, use_sampling, jitter_amount):
    """åˆ›å»ºæ•£ç‚¹å›¾çš„ç¼“å­˜å‡½æ•°ï¼ˆä½¿ç”¨å›ºå®šçš„é‡‡æ ·ç­–ç•¥å’Œå¯æ§åˆ¶çš„æŠ–åŠ¨ï¼‰"""
    try:
        # æ¸…ç†ä¹‹å‰çš„å›¾å½¢
        plt.close("all")

        # å‡†å¤‡æ•°æ®ï¼ˆå·²ç»æ¸…ç†è¿‡ç¼ºå¤±å€¼ï¼‰
        plot_data = df_clean[[x_col, y_col]]

        # ä½¿ç”¨å›ºå®šçš„é‡‡æ ·ç­–ç•¥
        if use_sampling and len(plot_data) > 2000:
            plot_data = plot_data.sample(n=2000, random_state=42)

        # è·å–åˆ—ç±»å‹
        numeric_cols = df_clean.select_dtypes(include=["number"]).columns.tolist()
        categorical_cols = df_clean.select_dtypes(include=["object", "category"]).columns.tolist()

        # æ•°æ®é¢„å¤„ç†ï¼šä¸ºåˆ†ç±»æ•°æ®æ·»åŠ æŠ–åŠ¨
        x_data = plot_data[x_col].copy()
        y_data = plot_data[y_col].copy()

        # ä¸ºåˆ†ç±»æ•°æ®æ·»åŠ æŠ–åŠ¨
        x_col_categorical = x_col in categorical_cols
        y_col_categorical = y_col in categorical_cols

        if x_col_categorical:
            # å°†åˆ†ç±»æ•°æ®è½¬æ¢ä¸ºæ•°å€¼ï¼Œå¹¶æ·»åŠ éšæœºæŠ–åŠ¨
            x_unique = x_data.unique()
            x_mapping = {val: i for i, val in enumerate(x_unique)}
            x_data = x_data.map(x_mapping)
            # æ·»åŠ å¯æ§åˆ¶çš„æŠ–åŠ¨
            x_jitter = np.random.normal(0, jitter_amount, len(x_data))
            x_data = x_data + x_jitter
        else:
            x_unique = None

        if y_col_categorical:
            # å°†åˆ†ç±»æ•°æ®è½¬æ¢ä¸ºæ•°å€¼ï¼Œå¹¶æ·»åŠ éšæœºæŠ–åŠ¨
            y_unique = y_data.unique()
            y_mapping = {val: i for i, val in enumerate(y_unique)}
            y_data = y_data.map(y_mapping)
            # æ·»åŠ å¯æ§åˆ¶çš„æŠ–åŠ¨
            y_jitter = np.random.normal(0, jitter_amount, len(y_data))
            y_data = y_data + y_jitter
        else:
            y_unique = None

        # åˆ›å»ºæ•£ç‚¹å›¾
        fig, ax = plt.subplots(figsize=(10, 8))

        # æ™®é€šæ•£ç‚¹å›¾
        ax.scatter(x_data, y_data, alpha=0.6, s=20)

        # è®¾ç½®åæ ‡è½´æ ‡ç­¾
        ax.set_xlabel(x_col)
        ax.set_ylabel(y_col)
        ax.set_title(f"{x_col} vs {y_col} æ•£ç‚¹å›¾")
        ax.grid(True, alpha=0.3)

        # ä¸ºåˆ†ç±»æ•°æ®è®¾ç½®åˆ»åº¦æ ‡ç­¾
        if x_col_categorical:
            ax.set_xticks(range(len(x_unique)))
            ax.set_xticklabels(x_unique, rotation=45, ha="right")

        if y_col_categorical:
            ax.set_yticks(range(len(y_unique)))
            ax.set_yticklabels(y_unique)

        # æ·»åŠ è¶‹åŠ¿çº¿ï¼ˆä»…å½“ä¸¤åˆ—éƒ½æ˜¯æ•°å€¼å‹æ—¶ï¼‰
        if x_col in numeric_cols and y_col in numeric_cols and len(plot_data) > 1:
            z = np.polyfit(plot_data[x_col], plot_data[y_col], 1)
            p = np.poly1d(z)
            ax.plot(plot_data[x_col], p(plot_data[x_col]), "r--", alpha=0.8, linewidth=2)

            # è®¡ç®—ç›¸å…³ç³»æ•°
            correlation = plot_data[x_col].corr(plot_data[y_col])
            ax.text(0.05, 0.95, f"ç›¸å…³ç³»æ•°: {correlation:.3f}",
                   transform=ax.transAxes, fontsize=10,
                   verticalalignment="top", bbox=dict(boxstyle="round", facecolor="wheat", alpha=0.8))

        plt.tight_layout()
        return fig, plot_data, x_col_categorical, y_col_categorical, numeric_cols
    except Exception:
        plt.close("all")  # ç¡®ä¿å‡ºé”™æ—¶ä¹Ÿæ¸…ç†å›¾å½¢
        return None, None, False, False, []


@st.cache_data
def process_data_for_visualization(df):
    """å¤„ç†æ•°æ®ç”¨äºå¯è§†åŒ–çš„ç¼“å­˜å‡½æ•°"""
    # å¤„ç†ç¼ºå¤±å€¼ï¼Œåˆ›å»ºå¹²å‡€çš„æ•°æ®å‰¯æœ¬
    df_clean = df.copy()

    # åªå¯¹ä»ç„¶æ˜¯objectç±»å‹çš„åˆ—è¿›è¡Œå­—ç¬¦ä¸²ç¼ºå¤±å€¼æ›¿æ¢
    string_missing_values = ["NA", "N/A", "null", "NULL", "missing", "Missing", "MISSING", "", " "]

    # å°†å­—ç¬¦ä¸²ç¼ºå¤±å€¼æ›¿æ¢ä¸ºNaNï¼ˆåªå¯¹objectç±»å‹ï¼‰
    for col in df_clean.columns:
        if df_clean[col].dtype == "object":
            for missing_val in string_missing_values:
                df_clean[col] = df_clean[col].replace(missing_val, pd.NA)

    # åˆ é™¤æ‰€æœ‰ç¼ºå¤±å€¼
    df_clean = df_clean.dropna()

    return df_clean


def display_data_info(df: pd.DataFrame) -> None:
    """æ˜¾ç¤ºæ•°æ®æ¡†çš„åŸºæœ¬ä¿¡æ¯"""
    with st.expander("æ•°æ®ä¿¡æ¯", expanded=False):
        # æ•°æ®ç±»å‹è½¬æ¢åŠŸèƒ½ - æ”¾åœ¨æœ€å¼€å§‹
        st.markdown("#### æ•°æ®ç±»å‹è°ƒæ•´")
        st.write("å¦‚æœæŸäº›åˆ—çš„æ•°æ®ç±»å‹ä¸æ­£ç¡®ï¼Œå¯ä»¥åœ¨è¿™é‡Œè¿›è¡Œè°ƒæ•´ï¼š")

        # æ•°æ®ç±»å‹è½¬æ¢é€‰é¡¹
        col1, col2 = st.columns(2)
        with col1:
            convert_col = st.selectbox("é€‰æ‹©è¦è½¬æ¢çš„åˆ—", df.columns.tolist())
        with col2:
            target_dtype = st.selectbox(
                "ç›®æ ‡æ•°æ®ç±»å‹",
                ["object", "int64", "float64", "datetime64", "category"],
                format_func=lambda x: {
                    "object": "æ–‡æœ¬ (object)",
                    "int64": "æ•´æ•° (int64)",
                    "float64": "æµ®ç‚¹æ•° (float64)",
                    "datetime64": "æ—¥æœŸæ—¶é—´ (datetime64)",
                    "category": "åˆ†ç±» (category)"
                }[x]
            )

        # è½¬æ¢æŒ‰é’®
        if st.button("ğŸ”„ è½¬æ¢æ•°æ®ç±»å‹"):
            try:
                original_dtype = df[convert_col].dtype

                if target_dtype == "datetime64":
                    # å°è¯•è‡ªåŠ¨è§£ææ—¥æœŸæ—¶é—´
                    df[convert_col] = pd.to_datetime(df[convert_col], errors="coerce")
                elif target_dtype == "int64":
                    # è½¬æ¢ä¸ºæ•´æ•°
                    df[convert_col] = pd.to_numeric(df[convert_col], errors="coerce").astype("Int64")
                elif target_dtype == "float64":
                    # è½¬æ¢ä¸ºæµ®ç‚¹æ•°
                    df[convert_col] = pd.to_numeric(df[convert_col], errors="coerce")
                elif target_dtype == "category":
                    # è½¬æ¢ä¸ºåˆ†ç±»
                    df[convert_col] = df[convert_col].astype("category")
                elif target_dtype == "object":
                    # è½¬æ¢ä¸ºæ–‡æœ¬
                    df[convert_col] = df[convert_col].astype("object")

                new_dtype = df[convert_col].dtype
                st.success(f"âœ… æˆåŠŸå°†åˆ— '{convert_col}' ä» {original_dtype} è½¬æ¢ä¸º {new_dtype}")

                # æ¸…é™¤ç¼“å­˜ï¼Œå› ä¸ºæ•°æ®ç±»å‹å‘ç”Ÿäº†å˜åŒ–
                process_data_for_visualization.clear()
                create_histogram.clear()
                create_bar_chart.clear()
                create_scatter_plot_simple.clear()

            except Exception as e:
                st.error(f"âŒ è½¬æ¢å¤±è´¥: {str(e)}")

        # æ˜¾ç¤ºå½“å‰æ•°æ®ç±»å‹ï¼ˆè½¬æ¢åï¼‰
        st.write("**å½“å‰æ•°æ®ç±»å‹ï¼š**")
        dtype_info = df.dtypes.to_frame("æ•°æ®ç±»å‹").reset_index()
        dtype_info.columns = ["åˆ—å", "æ•°æ®ç±»å‹"]
        # å°†dtypeå¯¹è±¡è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        dtype_info["æ•°æ®ç±»å‹"] = dtype_info["æ•°æ®ç±»å‹"].astype(str)
        st.dataframe(dtype_info, use_container_width=True)

        st.markdown("#### æ•°æ®æ ·æœ¬")
        st.dataframe(df.head())

        # æ£€æµ‹å„ç§å½¢å¼çš„ç¼ºå¤±å€¼
        st.markdown("#### æ•°æ®ç¼ºå¤±å€¼ä¿¡æ¯")

        # åˆ›å»ºæ•°æ®å‰¯æœ¬ï¼Œé¿å…ä¿®æ”¹åŸå§‹æ•°æ®
        df_clean = df.copy()

        # å®šä¹‰å¸¸è§çš„å­—ç¬¦ä¸²ç¼ºå¤±å€¼
        string_missing_values = ["NA", "N/A", "null", "NULL", "missing", "Missing", "MISSING", "", " "]

        # å°†å­—ç¬¦ä¸²ç¼ºå¤±å€¼æ›¿æ¢ä¸ºNaN
        for col in df_clean.columns:
            if df_clean[col].dtype == "object":
                for missing_val in string_missing_values:
                    df_clean[col] = df_clean[col].replace(missing_val, pd.NA)

        # ä½¿ç”¨isnull()æ£€æµ‹æ‰€æœ‰ç¼ºå¤±å€¼
        all_missing = df_clean.isnull().sum()
        st.dataframe(all_missing)


def display_data_visualization(df: pd.DataFrame) -> None:
    """æ˜¾ç¤ºæ•°æ®å¯è§†åŒ–"""
    with st.expander("æ•°æ®å¯è§†åŒ–", expanded=False):
        # ä½¿ç”¨ç¼“å­˜å‡½æ•°å¤„ç†æ•°æ®
        df_clean = process_data_for_visualization(df)

        if len(df_clean) == 0:
            st.warning("å¤„ç†ç¼ºå¤±å€¼åæ²¡æœ‰å‰©ä½™æ•°æ®ï¼Œæ— æ³•è¿›è¡Œå¯è§†åŒ–")
            return

        # è·å–æ‰€æœ‰åˆ—
        all_cols = df_clean.columns.tolist()
        numeric_cols = df_clean.select_dtypes(include=["number"]).columns.tolist()
        categorical_cols = df_clean.select_dtypes(include=["object", "category"]).columns.tolist()

        # åˆ†å¸ƒå›¾å¯è§†åŒ–
        st.markdown("#### æ•°æ®åˆ†å¸ƒå¯è§†åŒ–")

        # è®©ç”¨æˆ·é€‰æ‹©è¦æ˜¾ç¤ºçš„åˆ—
        st.write("é€‰æ‹©è¦æŸ¥çœ‹åˆ†å¸ƒçš„åˆ—ï¼ˆæœ€å¤š6åˆ—ï¼‰ï¼š")
        selected_cols = st.multiselect(
            "é€‰æ‹©åˆ—",
            options=all_cols,
            default=all_cols[:min(6, len(all_cols))],  # é»˜è®¤é€‰æ‹©å‰6åˆ—
            max_selections=6,
            format_func=lambda x: f"{x}" if x in numeric_cols else f"{x}" if x in categorical_cols else x
        )

        if len(selected_cols) > 0:
            # æ˜¾ç¤ºé€‰æ‹©çš„åˆ—ä¿¡æ¯
            st.write(f"å·²é€‰æ‹© {len(selected_cols)} åˆ—è¿›è¡Œå¯è§†åŒ–")

            # åˆ›å»ºå¤šåˆ—å¸ƒå±€
            cols = st.columns(min(3, len(selected_cols)))

            for i, col_name in enumerate(selected_cols):
                col_idx = i % 3

                with cols[col_idx]:
                    st.markdown(f"**{col_name}**")

                    if col_name in numeric_cols:
                        # æ•°å€¼å‹æ•°æ®ï¼šæ˜¾ç¤ºç›´æ–¹å›¾
                        fig = create_histogram(df_clean, col_name)
                        if fig is not None:
                            st.pyplot(fig)
                            plt.close(fig)
                        else:
                            st.write(f"æ— æ³•ç»˜åˆ¶ {col_name} çš„ç›´æ–¹å›¾")

                    elif col_name in categorical_cols:
                        # åˆ†ç±»æ•°æ®ï¼šæ˜¾ç¤ºæŸ±çŠ¶å›¾
                        fig = create_bar_chart(df_clean, col_name)
                        if fig is not None:
                            st.pyplot(fig)
                            plt.close(fig)
                        else:
                            st.write(f"æ— æ³•ç»˜åˆ¶ {col_name} çš„æŸ±çŠ¶å›¾")
        else:
            st.info("è¯·é€‰æ‹©è¦æŸ¥çœ‹çš„åˆ—")

        # æ•£ç‚¹å›¾å¯è§†åŒ–
        if len(all_cols) >= 2:
            st.markdown("#### æ•£ç‚¹å›¾åˆ†æ")

            # é‡‡æ ·è®¾ç½®
            max_sample_size = st.number_input(
                "æœ€å¤§é‡‡æ ·æ•°é‡ï¼ˆå¤§æ•°æ®é›†æ—¶ä½¿ç”¨ï¼‰",
                min_value=100,
                max_value=10000,
                value=1000,
                step=100,
                help="å½“æ•°æ®è¶…è¿‡æ­¤æ•°é‡æ—¶ï¼Œå°†éšæœºé‡‡æ ·ä»¥æå‡æ€§èƒ½"
            )

            # æŠ–åŠ¨è®¾ç½®
            jitter_amount = st.slider(
                "åˆ†ç±»æ•°æ®æŠ–åŠ¨å¤§å°",
                min_value=0.0,
                max_value=0.5,
                value=0.1,
                step=0.01,
                help="æ§åˆ¶åˆ†ç±»æ•°æ®çš„éšæœºæŠ–åŠ¨å¤§å°ï¼Œ0è¡¨ç¤ºæ— æŠ–åŠ¨ï¼Œæ•°å€¼è¶Šå¤§æŠ–åŠ¨è¶Šæ˜æ˜¾"
            )

            # é€‰æ‹©æ•£ç‚¹å›¾çš„Xå’ŒYè½´
            col1, col2 = st.columns(2)
            with col1:
                x_col = st.selectbox("é€‰æ‹©Xè½´åˆ—", all_cols, index=0)
            with col2:
                y_col = st.selectbox("é€‰æ‹©Yè½´åˆ—", all_cols, index=min(1, len(all_cols)-1))

            if x_col != y_col:
                try:
                    # å†³å®šæ˜¯å¦ä½¿ç”¨é‡‡æ ·
                    use_sampling = len(df_clean) > max_sample_size

                    # ä½¿ç”¨ç¼“å­˜å‡½æ•°åˆ›å»ºæ•£ç‚¹å›¾ï¼ˆä½¿ç”¨å›ºå®šçš„é‡‡æ ·ç­–ç•¥å’Œå¯æ§åˆ¶çš„æŠ–åŠ¨ï¼‰
                    fig, plot_data, x_col_categorical, y_col_categorical, numeric_cols = create_scatter_plot_simple(df_clean, x_col, y_col, use_sampling, jitter_amount)

                    if fig is not None:
                        # æ˜¾ç¤ºé‡‡æ ·ä¿¡æ¯
                        if use_sampling:
                            st.info(f"æ•°æ®é‡è¾ƒå¤§ï¼ˆ{len(df_clean)}æ¡ï¼‰ï¼Œå·²éšæœºé‡‡æ ·2000æ¡è¿›è¡Œå¯è§†åŒ–")

                        # æ˜¾ç¤ºæŠ–åŠ¨ä¿¡æ¯
                        if x_col_categorical or y_col_categorical:
                            st.info(f"åˆ†ç±»æ•°æ®æŠ–åŠ¨å¤§å°: {jitter_amount:.2f}")

                        st.pyplot(fig)
                        plt.close(fig)

                        # æ˜¾ç¤ºåŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
                        st.write("**æ•£ç‚¹å›¾ç»Ÿè®¡ä¿¡æ¯ï¼š**")
                        st.write(f"- æ•°æ®ç‚¹æ•°é‡: {len(plot_data)}")

                        if x_col in numeric_cols:
                            st.write(f"- Xè½´({x_col})èŒƒå›´: {plot_data[x_col].min():.2f} - {plot_data[x_col].max():.2f}")
                        else:
                            st.write(f"- Xè½´({x_col})ç±»åˆ«æ•°: {plot_data[x_col].nunique()}")

                        if y_col in numeric_cols:
                            st.write(f"- Yè½´({y_col})èŒƒå›´: {plot_data[y_col].min():.2f} - {plot_data[y_col].max():.2f}")
                        else:
                            st.write(f"- Yè½´({y_col})ç±»åˆ«æ•°: {plot_data[y_col].nunique()}")
                    else:
                        st.error("ç»˜åˆ¶æ•£ç‚¹å›¾æ—¶å‡ºé”™")

                except Exception as e:
                    st.error(f"ç»˜åˆ¶æ•£ç‚¹å›¾æ—¶å‡ºé”™: {str(e)}")
            else:
                st.warning("è¯·é€‰æ‹©ä¸åŒçš„åˆ—ä½œä¸ºXè½´å’ŒYè½´")
        else:
            st.info("éœ€è¦è‡³å°‘2åˆ—æ‰èƒ½ç»˜åˆ¶æ•£ç‚¹å›¾")


def get_metric_display_name(metric: str) -> str:
    """è·å–è¯„ä»·æŒ‡æ ‡çš„æ˜¾ç¤ºåç§°"""
    metric_names = {
        "accuracy": "å‡†ç¡®ç‡ (Accuracy)",
        "precision": "ç²¾ç¡®ç‡ (Precision)",
        "recall": "å¬å›ç‡ (Recall)",
        "f1": "F1åˆ†æ•° (F1-Score)",
        "mse": "å‡æ–¹è¯¯å·® (MSE)",
        "mae": "å¹³å‡ç»å¯¹è¯¯å·® (MAE)",
        "r2": "å†³å®šç³»æ•° (RÂ²)",
        "rmse": "å‡æ–¹æ ¹è¯¯å·® (RMSE)",
    }
    return metric_names.get(metric, metric)


def configure_metrics_ui(df, target_col=None, default_is_classification=None, config_data=None, default_name=None):
    """é…ç½®ä»»åŠ¡ç±»å‹ã€åŸºæœ¬ä¿¡æ¯å’Œè¯„ä»·æŒ‡æ ‡çš„UIç»„ä»¶

    Args:
        df: æ•°æ®æ¡†
        target_col: ç›®æ ‡åˆ—åï¼Œå¦‚æœä¸ºNoneåˆ™ä¼šæç¤ºç”¨æˆ·é€‰æ‹©
        default_is_classification: é»˜è®¤çš„ä»»åŠ¡ç±»å‹æ˜¯å¦ä¸ºåˆ†ç±»ï¼ŒNoneåˆ™é»˜è®¤ä¸ºåˆ†ç±»
        config_data: å¯é€‰çš„é»˜è®¤é…ç½®æ•°æ®
        default_name: é»˜è®¤ä»»åŠ¡åç§°ï¼Œå¦‚æœæ²¡æœ‰æä¾›config_dataæˆ–config_dataä¸­æ²¡æœ‰name

    Returns:
        tuple: (name, domain, domain_context, target_col, ignore_cols, is_classification, is_time_series, selected_metric, goal, ordinal_categories_list, date_feature, need_time, threshold)
               ä»»åŠ¡åç§°ã€é¢†åŸŸã€é¢†åŸŸä¸Šä¸‹æ–‡ã€ç›®æ ‡åˆ—ã€å¿½ç•¥çš„åˆ—ã€ä»»åŠ¡ç±»å‹ã€æ˜¯å¦ä¸ºæ—¶åºæ•°æ®ã€é€‰æ‹©çš„è¯„ä»·æŒ‡æ ‡ã€ç›®æ ‡å€¼ã€æœ‰åºç‰¹å¾ç±»åˆ«åˆ—è¡¨ã€æ—¥æœŸç‰¹å¾åˆ—ã€æ˜¯å¦éœ€è¦è§£ææ—¶é—´ã€æ—¶é—´é˜ˆå€¼
    """
    # å¦‚æœæ²¡æœ‰æä¾›config_dataï¼Œåˆå§‹åŒ–ä¸€ä¸ªç©ºå­—å…¸
    if config_data is None:
        config_data = {}

    # åŸºæœ¬ä¿¡æ¯é…ç½®
    name = st.text_input("âœï¸ ä»»åŠ¡åç§°", value=config_data.get("name", default_name or ""))
    domain = st.text_input("ğŸŒ é¢†åŸŸ", value=config_data.get("domain", "machine learning"))
    domain_context = st.text_input("ğŸ“š é¢†åŸŸä¸Šä¸‹æ–‡", value=config_data.get("domain_context", ""))

    # è®©ç”¨æˆ·é€‰æ‹©ç›®æ ‡åˆ—
    cols = df.columns.tolist()
    if target_col is None:
        target_col = st.selectbox(
            "ğŸ¯ é€‰æ‹©ç›®æ ‡åˆ—",
            cols,
            index=cols.index(config_data.get("target", cols[-1])) if "target" in config_data else len(cols) - 1,
        )

    # è®©ç”¨æˆ·é€‰æ‹©è¦å¿½ç•¥çš„åˆ—
    ignore_cols = st.multiselect("ğŸš« é€‰æ‹©è¦å¿½ç•¥çš„åˆ—", cols, default=config_data.get("ignored_features", []))

    # é€‰æ‹©ä»»åŠ¡ç±»å‹
    task_type_index = 0 if default_is_classification in (True, None) else 1
    is_classification = (
        st.radio("ğŸ“Š ä»»åŠ¡ç±»å‹", ["ğŸ” åˆ†ç±»", "ğŸ“ˆ å›å½’"], index=task_type_index, horizontal=True) == "ğŸ” åˆ†ç±»"
    )

    # æ ¹æ®ä»»åŠ¡ç±»å‹æä¾›å¯é€‰çš„è¯„ä»·æŒ‡æ ‡
    available_metrics = ["accuracy", "precision", "recall", "f1"] if is_classification else ["mse", "mae", "r2", "rmse"]
    selected_metric = st.selectbox("ğŸ“ é€‰æ‹©è¯„ä»·æŒ‡æ ‡", options=available_metrics, format_func=get_metric_display_name)

    # æ ¹æ®é€‰æ‹©çš„æŒ‡æ ‡è®¾ç½®åˆé€‚çš„ç›®æ ‡å€¼èŒƒå›´å’Œé»˜è®¤å€¼
    if selected_metric in ["accuracy", "precision", "recall", "f1"]:
        goal = st.slider(f"ğŸ¯ {get_metric_display_name(selected_metric)}ç›®æ ‡å€¼", 0.0, 1.0, 0.8)
    elif selected_metric in ["r2"]:
        goal = st.slider(f"ğŸ¯ {get_metric_display_name(selected_metric)}ç›®æ ‡å€¼", 0.0, 1.0, 0.6)
    elif selected_metric in ["mse", "mae", "rmse"]:
        # æ˜¾ç¤ºç›®æ ‡åˆ—çš„åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯ï¼Œå¸®åŠ©ç”¨æˆ·è®¾ç½®åˆç†çš„ç›®æ ‡å€¼
        target_stats = df[target_col].describe()
        st.write(f"ğŸ“Š ç›®æ ‡åˆ— '{target_col}' çš„åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯ï¼š")
        st.write(f"- å¹³å‡å€¼ï¼š{target_stats['mean']:.4f}")
        st.write(f"- æ ‡å‡†å·®ï¼š{target_stats['std']:.4f}")
        st.write(f"- æœ€å°å€¼ï¼š{target_stats['min']:.4f}")
        st.write(f"- æœ€å¤§å€¼ï¼š{target_stats['max']:.4f}")

        # ä½¿ç”¨number_inputè®©ç”¨æˆ·è‡ªç”±è¾“å…¥ç›®æ ‡å€¼
        st.info(f"ğŸ” è¯·æ ¹æ®æ•°æ®ç‰¹å¾è®¾ç½®{get_metric_display_name(selected_metric)}çš„ç›®æ ‡å€¼ï¼ˆè¶Šå°è¶Šå¥½ï¼‰")
        goal = st.number_input(
            "ğŸ¯ ç›®æ ‡å€¼",
            min_value=0.0,
            value=target_stats["std"],  # é»˜è®¤ä½¿ç”¨æ ‡å‡†å·®ä½œä¸ºå‚è€ƒå€¼
            format="%.4f",
        )

    with st.expander("âš™ï¸ é«˜çº§è®¾ç½®", expanded=False):
        ## ç‰¹å¾å·¥ç¨‹å»ºè®®
        st.write("ğŸ’¡ ç‰¹å¾å·¥ç¨‹å»ºè®®")
        requirements = st.text_input("ç‰¹å¾å·¥ç¨‹å»ºè®®", value=config_data.get("requirements", ""))

        # æœ‰åºåˆ†ç±»æ•°æ®
        ordinal_features_config = config_data.get("ordinal_features", [])
        # ä»é…ç½®ä¸­æå–ç‰¹å¾åç§°åˆ—è¡¨
        ordinal_feature_names = []
        for feature_dict in ordinal_features_config:
            # Each item is a dict with a single key (feature name)
            if feature_dict:  # Skip empty dicts
                ordinal_feature_names.extend(feature_dict.keys())

        ordinal_features = st.multiselect("ğŸ” é€‰æ‹©æœ‰åºåˆ†ç±»æ•°æ®", cols, default=ordinal_feature_names)

        # ä¸ºæ¯ä¸ªæœ‰åºç‰¹å¾è®¾ç½®ç±»åˆ«é¡ºåº
        ordinal_categories = {}
        if ordinal_features:
            st.write("ğŸ“Š è®¾ç½®æœ‰åºç‰¹å¾çš„ç±»åˆ«é¡ºåº")
            st.info("è¯·ä¸ºæ¯ä¸ªæœ‰åºç‰¹å¾æŒ‡å®šç±»åˆ«çš„é¡ºåºï¼Œç±»åˆ«ä¹‹é—´ç”¨é€—å·åˆ†éš”")

            # åˆ›å»ºå­—å…¸æ¥å­˜å‚¨é…ç½®ä¸­çš„é¡ºåºä¿¡æ¯
            config_orders = {}
            for feature_dict in ordinal_features_config:
                for feature, order in feature_dict.items():
                    config_orders[feature] = order

            for feature in ordinal_features:
                # è·å–è¯¥ç‰¹å¾çš„å”¯ä¸€å€¼
                unique_values = df[feature].unique().tolist()
                if len(unique_values) > 10:
                    st.warning(f"ğŸš¨ {feature} å¯èƒ½ä¸æ˜¯åˆ†ç±»ç‰¹å¾ï¼Œä¸ä½œä¸ºæœ‰åºç‰¹å¾å¤„ç†")
                    continue

                # å¦‚æœé…ç½®ä¸­æœ‰è¯¥ç‰¹å¾çš„é¡ºåºï¼Œä½¿ç”¨å®ƒä½œä¸ºé»˜è®¤å€¼ï¼Œå¦åˆ™ä½¿ç”¨æ•°æ®ä¸­çš„å”¯ä¸€å€¼
                if feature in config_orders:
                    default_order = ", ".join(map(str, config_orders[feature]))
                else:
                    default_order = ", ".join(map(str, unique_values))

                # è·å–ç”¨æˆ·è¾“å…¥çš„é¡ºåº
                order_input = st.text_input(
                    f"ğŸ“ {feature} çš„ç±»åˆ«é¡ºåº",
                    value=default_order,
                    help=f"å½“å‰æ‰€æœ‰å€¼: {', '.join(map(str, unique_values))}",
                )

                # è§£æè¾“å…¥çš„é¡ºåº
                if order_input:
                    order = [item.strip() for item in order_input.split(",")]
                    ordinal_categories[feature] = order

                    # æ˜¾ç¤ºé¢„è§ˆæ•ˆæœ
                    st.info(f"âœ… {feature} çš„ç±»åˆ«é¡ºåºå·²è®¾ç½®ä¸º: {order}")

        # å°†ordinal_categoriesè½¬æ¢ä¸ºæ¨¡å‹ä»£ç æœŸæœ›çš„æ ¼å¼ï¼š[{feature1: [order1]}, {feature2: [order2]}]
        ordinal_categories_list = []
        for feature, order in ordinal_categories.items():
            ordinal_categories_list.append({feature: order})

        # æƒ³è¦è§£ææ—¶é—´çš„åˆ—
        date_feature = config_data.get("date_feature") if config_data.get("date_feature") != "" else cols[1]
        date_options = ["ä¸é€‰æ‹©"] + cols
        date_index = cols.index(date_feature) + 1 if date_feature in cols else 0
        date_feature = st.selectbox("ğŸ•’ é€‰æ‹©éœ€è¦è§£ææ—¥æœŸæ—¶é—´çš„åˆ—", date_options, index=0)
        date_feature = "" if date_feature == "ä¸é€‰æ‹©" else date_feature

        if date_feature != "":
            need_time = st.radio("ğŸ•’ æ˜¯å¦éœ€è¦è§£ææ—¶é—´", ["âœ… æ˜¯", "âŒ å¦"], index=1, horizontal=True) == "âœ… æ˜¯"

            # é€‰æ‹©æŒ‰ç…§æ—¥æœŸæ—¶é—´åˆ†å‰²çš„é˜ˆå€¼
            if "need_time" in locals() and need_time:
                date_col = st.columns(1)[0]
                threshold_date = date_col.date_input(
                    "ğŸ•’ é€‰æ‹©æŒ‰ç…§æ—¥æœŸåˆ†å‰²çš„é˜ˆå€¼æ—¥æœŸ", value=pd.Timestamp("2015-09-01").date()
                )

                time_cols = st.columns(3)
                threshold_hour = time_cols[0].number_input("å°æ—¶", min_value=0, max_value=23, value=0, step=1)
                threshold_minute = time_cols[1].number_input("åˆ†é’Ÿ", min_value=0, max_value=59, value=0, step=1)
                threshold_second = time_cols[2].number_input("ç§’", min_value=0, max_value=59, value=0, step=1)

                # åˆå¹¶æ—¥æœŸå’Œæ—¶é—´
                threshold = pd.Timestamp(
                    year=threshold_date.year,
                    month=threshold_date.month,
                    day=threshold_date.day,
                    hour=threshold_hour,
                    minute=threshold_minute,
                    second=threshold_second,
                )

                # å°†Timestampè½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                threshold_dict = {
                    "year": threshold_date.year,
                    "month": threshold_date.month,
                    "day": threshold_date.day,
                    "hour": threshold_hour,
                    "minute": threshold_minute,
                    "second": threshold_second,
                }
            else:
                threshold = st.date_input("ğŸ•’ é€‰æ‹©æŒ‰ç…§æ—¥æœŸæ—¶é—´åˆ†å‰²çš„é˜ˆå€¼", value=pd.Timestamp("2015-09-01"))

                # å°†dateè½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                threshold_dict = {
                    "year": threshold.year,
                    "month": threshold.month,
                    "day": threshold.day,
                }

        # é€‰æ‹©æ˜¯å¦ä¸ºæ—¶åºæ•°æ®
        is_time_series = st.radio("ğŸ•’ æ˜¯å¦ä¸ºæ—¶åºé—®é¢˜", ["âœ… æ˜¯", "âŒ å¦"], index=1, horizontal=True) == "âœ… æ˜¯"
        if is_time_series:
            if date_feature == "":
                st.error("ğŸš¨ è¯·é€‰æ‹©éœ€è¦è§£ææ—¥æœŸæ—¶é—´çš„åˆ—")
                return

    return (
        name,
        domain,
        domain_context,
        target_col,
        ignore_cols,
        is_classification,
        is_time_series,
        selected_metric,
        goal,
        ordinal_categories_list,
        date_feature,
        need_time if "need_time" in locals() else False,
        threshold_dict if is_time_series and "threshold_dict" in locals() else {},
        requirements,
    )


def vision_uploader() -> Dict[str, Any]:
    """è§†è§‰åŸºç¡€è®¾ç½®ç»„ä»¶"""
    st.subheader("åŸºç¡€è®¾ç½®")

    upload_type = st.radio("é€‰æ‹©ä¸Šä¼ æ–¹å¼", ["ğŸ“ æ–‡ä»¶å¤¹ä¸Šä¼ ", "ğŸ’¾ ç¤ºä¾‹æ•°æ®é›†"])

    if upload_type == "ğŸ“ æ–‡ä»¶å¤¹ä¸Šä¼ ":
        # é€‰æ‹©æ ‡æ³¨æ–¹å¼
        label_type = st.radio("é€‰æ‹©æ ‡æ³¨æ–¹å¼", ["ğŸ“ æ–‡ä»¶å¤¹ç»“æ„æ ‡æ³¨", "ğŸ“„ CSVæ–‡ä»¶æ ‡æ³¨"], horizontal=True)
        
        if label_type == "ğŸ“ æ–‡ä»¶å¤¹ç»“æ„æ ‡æ³¨":
            st.info("ğŸ“‚ è¯·é€‰æ‹©åŒ…å«å›¾ç‰‡çš„æ–‡ä»¶å¤¹ï¼Œæ¯ä¸ªå­æ–‡ä»¶å¤¹åä¸ºç±»åˆ«å")
            train_path = st.text_input("ğŸ” è®­ç»ƒæ•°æ®è·¯å¾„")
            valid_path = st.text_input("ğŸ” éªŒè¯æ•°æ®è·¯å¾„ï¼ˆå¯é€‰ï¼‰")

            if train_path:
                # åŸºç¡€é…ç½®
                st.subheader("æ¨¡å‹é…ç½®")
                batch_size = st.number_input("æ‰¹æ¬¡å¤§å°", 1, 128, 4)
                model = st.selectbox("æ¨¡å‹", ["resnet18", "resnet34", "resnet50"], index=0)
                valid_pct = st.slider("éªŒè¯é›†æ¯”ä¾‹", 0.0, 0.5, 0.2)
                num_workers = st.number_input("æ•°æ®åŠ è½½çº¿ç¨‹æ•°", 0, 16, 4)
                size = st.number_input("å›¾ç‰‡å¤§å°", 16, 256, 128)

                # åˆ›å»ºé…ç½®æ•°æ®
                config_data = {
                    "path": train_path,
                    "valid_path": valid_path,
                    "name": Path(train_path).name,
                    "task_type": TaskType.VISION_SINGLE_LABEL.value,
                    "model": model,
                    "batch_size": batch_size,
                    "valid_pct": valid_pct,
                    "num_workers": num_workers,
                    "size": size,
                    "train_folder": None,
                    "valid_folder": None,
                    "device": "cuda" if torch.cuda.is_available() else "cpu",
                }
                return create_dl_config(config_data)
        
        else:  # CSVæ–‡ä»¶æ ‡æ³¨
            st.info("ğŸ“„ è¯·é€‰æ‹©åŒ…å«å›¾ç‰‡çš„æ–‡ä»¶å¤¹ï¼Œå¹¶æä¾›CSVæ ‡æ³¨æ–‡ä»¶")
            
            # æ•°æ®è·¯å¾„é…ç½®
            data_path = st.text_input("ğŸ” æ•°æ®æ ¹ç›®å½•è·¯å¾„", help="åŒ…å«å›¾ç‰‡æ–‡ä»¶å¤¹å’ŒCSVæ ‡æ³¨æ–‡ä»¶çš„æ ¹ç›®å½•")
            
            if data_path:
                # CSVæ–‡ä»¶é…ç½®
                st.subheader("CSVæ ‡æ³¨æ–‡ä»¶é…ç½®")
                col1, col2 = st.columns(2)
                
                with col1:
                    folder = st.text_input("ğŸ“ å›¾ç‰‡æ–‡ä»¶å¤¹åç§°", value="", help="å›¾ç‰‡æ‰€åœ¨å­æ–‡ä»¶å¤¹åç§°ï¼Œå¦‚æœå›¾ç‰‡åœ¨æ ¹ç›®å½•è¯·ç•™ç©º")
                    csv_file = st.text_input("ğŸ“„ CSVæ–‡ä»¶å", value="labels.csv", help="CSVæ ‡æ³¨æ–‡ä»¶å")
                
                with col2:
                    image_col = st.text_input("ğŸ–¼ï¸ å›¾ç‰‡åˆ—å", value="image", help="CSVæ–‡ä»¶ä¸­å›¾ç‰‡æ–‡ä»¶åæ‰€åœ¨åˆ—å")
                    label_col = st.text_input("ğŸ·ï¸ æ ‡ç­¾åˆ—å", value="label", help="CSVæ–‡ä»¶ä¸­æ ‡ç­¾æ‰€åœ¨åˆ—å")
                
                # ä»»åŠ¡ç±»å‹é€‰æ‹©
                task_type_option = st.radio("ä»»åŠ¡ç±»å‹", ["ğŸ” å•æ ‡ç­¾åˆ†ç±»", "ğŸ·ï¸ å¤šæ ‡ç­¾åˆ†ç±»"], horizontal=True)
                
                # åŸºç¡€é…ç½®
                st.subheader("æ¨¡å‹é…ç½®")
                batch_size = st.number_input("æ‰¹æ¬¡å¤§å°", 1, 128, 4)
                model = st.selectbox("æ¨¡å‹", ["resnet18", "resnet34", "resnet50"], index=0)
                valid_pct = st.slider("éªŒè¯é›†æ¯”ä¾‹", 0.0, 0.5, 0.2)
                num_workers = st.number_input("æ•°æ®åŠ è½½çº¿ç¨‹æ•°", 0, 16, 4)
                size = st.number_input("å›¾ç‰‡å¤§å°", 16, 256, 128)

                # ç¡®å®šä»»åŠ¡ç±»å‹
                if task_type_option == "ğŸ·ï¸ å¤šæ ‡ç­¾åˆ†ç±»":
                    task_type = TaskType.VISION_MULTI_LABEL.value
                else:
                    task_type = TaskType.VISION_CSV.value

                # åˆ›å»ºé…ç½®æ•°æ®
                config_data = {
                    "path": data_path,
                    "name": Path(data_path).name,
                    "task_type": task_type,
                    "model": model,
                    "batch_size": batch_size,
                    "valid_pct": valid_pct,
                    "num_workers": num_workers,
                    "size": size,
                    "device": "cuda" if torch.cuda.is_available() else "cpu",
                    "folder": f"'{folder}'" if folder else None,
                    "csv_file": f"'{csv_file}'",
                    "image_col": f"'{image_col}'",
                    "label_col": f"'{label_col}'",
                }
                return create_dl_config(config_data)

    else:  # ç¤ºä¾‹æ•°æ®é›†
        vision_configs = {
            "ç†Šåˆ†ç±»ï¼ˆå•æ ‡ç­¾ï¼‰": bears_single_label_config,
            "ç†Šåˆ†ç±»ï¼ˆå‡½æ•°æ ‡æ³¨ï¼‰": bears_func_config,
            "ç†Šåˆ†ç±»ï¼ˆæ­£åˆ™æ ‡æ³¨ï¼‰": bears_re_config,
            "MNISTæ‰‹å†™æ•°å­—": mnist_csv_config,
            "PASCALå¤šæ ‡ç­¾åˆ†ç±»": multi_label_config,
        }

        selected_dataset = st.selectbox("ğŸ” é€‰æ‹©ç¤ºä¾‹æ•°æ®é›†", list(vision_configs.keys()))
        default_config = vision_configs[selected_dataset]

        # æ˜¾ç¤ºæ•°æ®é›†åŸºæœ¬ä¿¡æ¯
        st.write(f"æ•°æ®é›†: {default_config['name']}")
        st.write(f"æ•°æ®è·¯å¾„: {default_config['path']}")
        st.write(f"ä»»åŠ¡ç±»å‹: {default_config['task_type']}")

        # åŸºç¡€é…ç½®
        batch_size = st.number_input("æ‰¹æ¬¡å¤§å°", 1, 128, default_config["batch_size"])
        model = st.selectbox(
            "æ¨¡å‹",
            ["resnet18", "resnet34", "resnet50"],
            index=["resnet18", "resnet34", "resnet50"].index(default_config["model"]),
        )
        valid_pct = st.slider("éªŒè¯é›†æ¯”ä¾‹", 0.0, 0.5, default_config["valid_pct"])
        num_workers = st.number_input("æ•°æ®åŠ è½½çº¿ç¨‹æ•°", 0, 16, default_config["num_workers"])
        size = st.number_input("å›¾ç‰‡å¤§å°", 16, 256, default_config["size"])

        # æ ¹æ®ä»»åŠ¡ç±»å‹æ˜¾ç¤ºä¸åŒçš„é…ç½®é€‰é¡¹
        if default_config["task_type"] == TaskType.VISION_CSV.value:
            folder = st.text_input("æ•°æ®é›†æ–‡ä»¶å¤¹", value=default_config.get("folder", None))
            csv_file = st.text_input("CSVæ–‡ä»¶å", value=default_config["csv_file"])
            image_col = st.text_input("å›¾ç‰‡åˆ—å", value=default_config["image_col"])
            label_col = st.text_input("æ ‡ç­¾åˆ—å", value=default_config["label_col"])
            valid_col = st.text_input("éªŒè¯é›†åˆ—å", value=default_config.get("valid_col", None))
            delimiter = st.text_input("åˆ†éš”ç¬¦", value=default_config.get("delimiter", None))
            label_delim = st.text_input("æ ‡ç­¾åˆ†éš”ç¬¦", value=default_config.get("label_delim", None))
            extra_config = {
                "folder": folder,
                "csv_file": csv_file,
                "image_col": image_col,
                "label_col": label_col,
                "valid_col": valid_col,
                "delimiter": delimiter,
                "label_delim": label_delim,
            }
        elif default_config["task_type"] == TaskType.VISION_FUNC.value:
            label_func = st.text_input("æ ‡æ³¨å‡½æ•°", value=default_config["label_func"])
            extra_config = {"label_func": label_func}
        elif default_config["task_type"] == TaskType.VISION_RE.value:
            pat = st.text_input("æ ‡æ³¨æ­£åˆ™è¡¨è¾¾å¼", value=default_config["pat"])
            extra_config = {"pat": pat}
        elif default_config["task_type"] == TaskType.VISION_MULTI_LABEL.value:
            folder = st.text_input("æ•°æ®é›†æ–‡ä»¶å¤¹", value=default_config.get("folder", None))
            csv_file = st.text_input("CSVæ–‡ä»¶å", value=default_config["csv_file"])
            image_col = st.text_input("å›¾ç‰‡åˆ—å", value=default_config["image_col"])
            label_col = st.text_input("æ ‡ç­¾åˆ—å", value=default_config["label_col"])
            valid_col = st.text_input("éªŒè¯é›†åˆ—å", value=default_config.get("valid_col", None))
            delimiter = st.text_input("åˆ†éš”ç¬¦", value=default_config.get("delimiter", None))
            label_delim = st.text_input("æ ‡ç­¾åˆ†éš”ç¬¦", value=default_config.get("label_delim", None))
            extra_config = {
                "folder": folder,
                "csv_file": csv_file,
                "image_col": image_col,
                "label_col": label_col,
                "valid_col": valid_col,
                "delimiter": delimiter,
                "label_delim": label_delim,
            }
        else:  # VISION_SINGLE_LABEL
            train_folder = st.text_input("è®­ç»ƒé›†æ–‡ä»¶å¤¹", value=default_config.get("train_folder", None))
            valid_folder = st.text_input("éªŒè¯é›†æ–‡ä»¶å¤¹", value=default_config.get("valid_folder", None))
            extra_config = {
                "train_folder": train_folder,
                "valid_folder": valid_folder,
            }

        # åˆ›å»ºé…ç½®æ•°æ®
        config_data = {
            "path": default_config["path"],
            "name": default_config["name"],
            "task_type": default_config["task_type"],
            "batch_size": batch_size,
            "model": model,
            "valid_pct": valid_pct,
            "num_workers": num_workers,
            "device": "cuda" if torch.cuda.is_available() else "cpu",
            "size": size,
            **extra_config,
        }
        return create_dl_config(config_data)

    return {}


def nlp_uploader() -> Dict[str, Any]:
    """NLPåŸºç¡€è®¾ç½®ç»„ä»¶"""
    st.subheader("åŸºç¡€è®¾ç½®")

    # åªé€‰æ‹©ä»»åŠ¡ç±»å‹
    nlp_configs = {
        "æƒ…æ„Ÿåˆ†ææ¨ç†": sentiment_inference_config,
        "å‘½åå®ä½“è¯†åˆ«": ner_inference_config,
        "è¯­ä¹‰åŒ¹é…": semantic_match_inference_config,
        "æƒ…æ„Ÿåˆ†ç±»è®­ç»ƒ": sentiment_classifier_trainer_config,
    }

    selected_dataset = st.selectbox("ğŸ” é€‰æ‹©ä»»åŠ¡", list(nlp_configs.keys()))
    default_config = nlp_configs[selected_dataset]

    # æ˜¾ç¤ºä»»åŠ¡ç±»å‹
    st.write(f"ä»»åŠ¡ç±»å‹: {default_config['task_type']}")

    # å¦‚æœæ˜¯"æƒ…æ„Ÿåˆ†ç±»è®­ç»ƒ"ï¼Œæä¾›æ•°æ®è·¯å¾„è¾“å…¥
    if selected_dataset == "æƒ…æ„Ÿåˆ†ç±»è®­ç»ƒ":
        data_path = st.text_input("ğŸ” æ•°æ®è·¯å¾„", value=default_config.get("path", ""))
        if data_path:
            text_column = st.text_input("æ–‡æœ¬åˆ—å", value=default_config.get("text_column", ""))
            label_column = st.text_input("æ ‡ç­¾åˆ—å", value=default_config.get("label_column", ""))
            num_labels = st.number_input("ç±»åˆ«æ•°é‡", 2, 100, default_config.get("num_labels", 2))
            num_epochs = st.number_input("è®­ç»ƒè½®æ•°", 1, 100, default_config.get("num_epochs", 1))

            # åˆ›å»ºé…ç½®æ•°æ®
            config_data = {
                "path": data_path,
                "name": default_config.get("name", selected_dataset),
                "task_type": default_config["task_type"],  # ä½¿ç”¨é¢„è®¾çš„ä»»åŠ¡ç±»å‹
                "model": default_config.get("model", "bert-base-chinese"),
                "batch_size": default_config.get("batch_size", 32),
                "num_epochs": num_epochs,
                "text_column": text_column,
                "label_column": label_column,
                "num_labels": num_labels,
            }

            if "label_mapping" in default_config:
                config_data["label_mapping"] = default_config["label_mapping"]

            return create_dl_config(config_data)

    else:  # æ¨ç†ä»»åŠ¡
        if "input" in default_config:
            input_text = st.text_input("è¾“å…¥æ–‡æœ¬", value=default_config["input"])
            config_data = {
                "name": selected_dataset,  # æ·»åŠ ä»»åŠ¡åç§°
                "task_type": default_config["task_type"],  # ä½¿ç”¨é¢„è®¾çš„ä»»åŠ¡ç±»å‹
                "model": default_config.get("model", "bert-base-chinese"),
                "input": input_text,
            }
        elif "input1" in default_config:
            input1 = st.text_input("è¾“å…¥æ–‡æœ¬1", value=default_config["input1"])
            input2 = st.text_input("è¾“å…¥æ–‡æœ¬2", value=default_config["input2"])
            config_data = {
                "name": selected_dataset,  # æ·»åŠ ä»»åŠ¡åç§°
                "task_type": default_config["task_type"],  # ä½¿ç”¨é¢„è®¾çš„ä»»åŠ¡ç±»å‹
                "model": default_config.get("model", "bert-base-chinese"),
                "input1": input1,
                "input2": input2,
            }
        return create_dl_config(config_data)

    return {}


def collab_uploader() -> Dict[str, Any]:
    """æ¨èç³»ç»ŸåŸºç¡€è®¾ç½®ç»„ä»¶"""
    st.subheader("åŸºç¡€è®¾ç½®")

    upload_type = st.radio("é€‰æ‹©ä¸Šä¼ æ–¹å¼", ["ğŸ’¾ ç¤ºä¾‹æ•°æ®é›†"])

    if upload_type == "ğŸ’¾ ç¤ºä¾‹æ•°æ®é›†":
        default_config = collab_config

        # æ˜¾ç¤ºæ•°æ®é›†ä¿¡æ¯
        st.write(f"ä»»åŠ¡ç±»å‹: {default_config['task_type']}")
        st.write(f"æ•°æ®è·¯å¾„: {default_config['path']}")

        # å¯è°ƒæ•´çš„é…ç½®
        user_name = st.text_input("ç”¨æˆ·åˆ—å", value=default_config["user_name"])
        item_name = st.text_input("ç‰©å“åˆ—å", value=default_config["item_name"])
        rating_name = st.text_input("è¯„åˆ†åˆ—å", value=default_config["rating_name"])
        valid_pct = st.slider("éªŒè¯é›†æ¯”ä¾‹", 0.0, 0.5, default_config["valid_pct"])
        y_range = st.slider("è¯„åˆ†èŒƒå›´", 0.0, 10.0, (default_config["y_range_min"], default_config["y_range_max"]))

        # åˆ›å»ºé…ç½®æ•°æ®
        config_data = {
            "path": default_config["path"],
            "name": "MovieLens",
            "task_type": TaskType.COLLABORATIVE.value,
            "model": default_config.get("model", "collaborative_filtering"),
            "batch_size": default_config.get("batch_size", 32),
            "num_epochs": default_config.get("num_epochs", 3),
            "user_name": user_name,
            "item_name": item_name,
            "rating_name": rating_name,
            "valid_pct": valid_pct,
            "y_range_min": y_range[0],
            "y_range_max": y_range[1],
        }
        return create_dl_config(config_data)

    return {}


def ml_uploader() -> Dict[str, Any]:
    """æœºå™¨å­¦ä¹ åŸºç¡€è®¾ç½®ç»„ä»¶"""
    st.subheader("åŸºç¡€è®¾ç½®")

    upload_type = st.radio("é€‰æ‹©æ•°æ®é›†", ["ğŸ“¤ ä¸Šä¼ æ•°æ®é›†", "ğŸ’¾ ä½¿ç”¨ç¤ºä¾‹æ•°æ®é›†"])

    if upload_type == "ğŸ“¤ ä¸Šä¼ æ•°æ®é›†":
        # Update supported file types based on load_data capabilities
        supported_types = ["csv", "xls", "xlsx", "json", "html", "parquet", "pkl", "h5", "txt", "xml", "db"]
        file = st.file_uploader("ğŸ“ ä¸Šä¼ æ•°æ®æ–‡ä»¶", type=supported_types)

        if file:
            # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
            save_path = get_tmp_folder() / "from_user" / "ml"
            save_path.mkdir(exist_ok=True, parents=True)
            file_path = save_path / file.name
            with open(file_path, "wb") as f:
                f.write(file.getbuffer())

            # æ ¹æ®æ–‡ä»¶ç±»å‹è®¾ç½®é¢å¤–å‚æ•°
            file_extension = file_path.suffix.lower().strip(".")
            extra_params = {}

            # æ˜¾ç¤ºç‰¹å®šæ–‡ä»¶ç±»å‹çš„é¢å¤–é€‰é¡¹
            if file_extension in ["txt", "csv"]:
                delimiter_options = {
                    ",": "é€—å· (,)",
                    ";": "åˆ†å· (;)",
                    "\t": "åˆ¶è¡¨ç¬¦ (Tab)",
                    "|": "ç«–çº¿ (|)",
                    " ": "ç©ºæ ¼ (Space)",
                    "custom": "è‡ªå®šä¹‰åˆ†éš”ç¬¦",
                }
                default_delimiter = "," if file_extension == "csv" else "\t"
                delimiter_choice = st.selectbox(
                    "ğŸ“ åˆ†éš”ç¬¦",
                    options=list(delimiter_options.keys()),
                    format_func=lambda x: delimiter_options[x],
                    index=list(delimiter_options.keys()).index(default_delimiter),
                )

                if delimiter_choice == "custom":
                    custom_delimiter = st.text_input("âœï¸ è¯·è¾“å…¥è‡ªå®šä¹‰åˆ†éš”ç¬¦")
                    if custom_delimiter:  # ç¡®ä¿ç”¨æˆ·è¾“å…¥äº†åˆ†éš”ç¬¦
                        delimiter = custom_delimiter
                    else:
                        st.warning("âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆçš„åˆ†éš”ç¬¦")
                        delimiter = default_delimiter  # å¦‚æœç”¨æˆ·æ²¡æœ‰è¾“å…¥ï¼Œä½¿ç”¨é»˜è®¤åˆ†éš”ç¬¦
                else:
                    delimiter = delimiter_choice

                extra_params["delimiter"] = delimiter
            elif file_extension == "h5":
                key = st.text_input("ğŸ”‘ æ•°æ®é”®", value="df")
                extra_params["key"] = key
            elif file_extension == "db":
                query = st.text_input("ğŸ” SQLæŸ¥è¯¢", value="SELECT * FROM table_name")
                extra_params["query"] = query

            try:
                # ä½¿ç”¨load_dataåŠ è½½æ•°æ®
                df = load_data(path=file_path, **extra_params)
                display_data_info(df)
                display_data_visualization(df)

                # è®©ç”¨æˆ·è®¾ç½®åŸºæœ¬é…ç½®å¹¶é…ç½®ä»»åŠ¡ç±»å‹å’Œè¯„ä»·æŒ‡æ ‡
                (
                    name,
                    domain,
                    domain_context,
                    target_col,
                    ignore_cols,
                    is_classification,
                    is_time_series,
                    selected_metric,
                    goal,
                    ordinal_categories_list,
                    date_feature,
                    need_time,
                    threshold,
                    requirements,
                ) = configure_metrics_ui(df, None, None, {}, file.name.split(".")[0].replace(" ", "_"))

                # åˆ›å»ºé…ç½®æ•°æ®
                config_data = {
                    "delimiter": extra_params.get("delimiter", ","),
                    "path": str(file_path),
                    "target": target_col,
                    "ignored_features": ignore_cols,
                    "classification": is_classification,
                    "time_series": is_time_series,
                    "metrics": selected_metric,
                    "selected_metric": selected_metric,
                    "name": name,
                    "domain": domain,
                    "domain_context": domain_context,
                    "goal": goal,
                    "task_type": TaskType.ML.value,
                    "ordinal_features": ordinal_categories_list,
                    "date_feature": date_feature,
                    "need_time": need_time,
                    "threshold": threshold,
                    "requirements": requirements,
                }

                # ä½¿ç”¨create_ml_configåˆ›å»ºæ ‡å‡†é…ç½®
                return create_ml_config(config_data)

            except Exception as e:
                st.error(f"åŠ è½½æ•°æ®æ—¶å‡ºé”™: {str(e)}")
                return {}

    else:  # ç¤ºä¾‹æ•°æ®é›†
        dataset_configs = {
            "iris": iris_config_data,
            "titanic": titanic_config_data,
            "house": house_config_data,
            "garment": garment_config_data,
        }
        selected_dataset = st.selectbox("ğŸ” é€‰æ‹©ç¤ºä¾‹æ•°æ®é›†", list(dataset_configs.keys()))

        if selected_dataset:
            if selected_dataset == "iris":
                with st.expander("ğŸ” äº†è§£irisæ•°æ®é›†", expanded=False):
                    st.write(
                        "irisæ•°æ®é›†æ˜¯ä¸€ä¸ªç»å…¸çš„æœºå™¨å­¦ä¹ æ•°æ®é›†ï¼Œç”¨äºåˆ†ç±»ä»»åŠ¡ã€‚å®ƒåŒ…å«150ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬æœ‰4ä¸ªç‰¹å¾ï¼ˆèŠ±è¼é•¿åº¦ã€èŠ±è¼å®½åº¦ã€èŠ±ç“£é•¿åº¦ã€èŠ±ç“£å®½åº¦ï¼‰å’Œä¸€ä¸ªç›®æ ‡å˜é‡ï¼ˆèŠ±çš„ç§ç±»ï¼‰ã€‚"
                    )
                    st.write("- èŠ±è¼é•¿åº¦ (sepal length)")
                    st.write("- èŠ±è¼å®½åº¦ (sepal width)")
                    st.write("- èŠ±ç“£é•¿åº¦ (petal length)")
                    st.write("- èŠ±ç“£å®½åº¦ (petal width)")
                    st.write("**ç›®æ ‡å˜é‡ï¼š**")
                    st.write("- èŠ±çš„ç§ç±» (iris)")

                if not os.path.exists(iris_config_data["path"]):
                    with st.status("è‡ªåŠ¨ä¸‹è½½irisæ•°æ®é›†"):
                        from sklearn import datasets

                        iris = datasets.load_iris()
                        iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)
                        iris_df["iris"] = iris.target_names[iris.target]
                        iris_df.to_csv(iris_config_data["path"], index=False)

            if selected_dataset == "titanic":
                with st.expander("ğŸ” äº†è§£titanicæ•°æ®é›†", expanded=False):
                    st.markdown("https://seaborn.pydata.org/generated/seaborn.load_dataset.html")

                if not os.path.exists(titanic_config_data["path"]):
                    with st.status("è‡ªåŠ¨ä¸‹è½½titanicæ•°æ®é›†"):
                        from seaborn import load_dataset

                        titanic = load_dataset("titanic")
                        titanic.to_csv(titanic_config_data["path"], index=False)

            if selected_dataset == "house":
                with st.expander("ğŸ” äº†è§£houseæ•°æ®é›†", expanded=False):
                    st.markdown("https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset")

                if not os.path.exists(house_config_data["path"]):
                    with st.status("è‡ªåŠ¨ä¸‹è½½houseæ•°æ®é›†"):
                        from sklearn.datasets import fetch_california_housing

                        housing = fetch_california_housing(as_frame=True)
                        housing_df = pd.concat([housing.data, housing.target], axis=1)
                        housing_df.to_csv(house_config_data["path"], index=False)

            if selected_dataset == "garment":
                with st.expander("ğŸ” äº†è§£garmentæ•°æ®é›†", expanded=False):
                    st.markdown(
                        "Productivity Prediction of Garment Employees [Dataset]. (2020). UCI Machine Learning Repository. <https://doi.org/10.24432/C51S6D>"
                    )

            config_data = dataset_configs[selected_dataset]
            df = load_example_data(selected_dataset)
            display_data_info(df)
            display_data_visualization(df)

            # é…ç½®ä»»åŠ¡ç±»å‹å’Œè¯„ä»·æŒ‡æ ‡ï¼Œä½¿ç”¨å·²æœ‰é…ç½®ä½œä¸ºé»˜è®¤å€¼
            (
                name,
                domain,
                domain_context,
                target_col,
                ignore_cols,
                is_classification,
                is_time_series,
                selected_metric,
                goal,
                ordinal_categories_list,
                date_feature,
                need_time,
                threshold,
                requirements,
            ) = configure_metrics_ui(
                df, None, config_data.get("classification"), config_data, config_data.get("name")
            )

            # åˆ›å»ºé…ç½®æ•°æ®
            config_data = {
                "delimiter": ",",
                "path": config_data["path"],
                "target": target_col,
                "ignored_features": ignore_cols,
                "classification": is_classification,
                "time_series": is_time_series,
                "metrics": selected_metric,
                "selected_metric": selected_metric,
                "name": name,
                "domain": domain,
                "domain_context": domain_context,
                "goal": goal,
                "task_type": TaskType.ML.value,
                "ordinal_features": ordinal_categories_list,
                "date_feature": date_feature,
                "need_time": need_time,
                "threshold": threshold,
                "requirements": requirements,
            }

            # ä½¿ç”¨create_ml_configåˆ›å»ºæ ‡å‡†é…ç½®
            return create_ml_config(config_data)

    return {}
